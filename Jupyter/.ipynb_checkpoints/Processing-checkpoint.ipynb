{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product query failure: bot detected\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from lxml import html  \n",
    "import json\n",
    "import requests\n",
    "import json\n",
    "from dateutil import parser as dateparser\n",
    "from time import sleep\n",
    "import re\n",
    "\n",
    "def get_products(product = \"sprite\"):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.9) Gecko/2008051206 Firefox/3.0'}\n",
    "    amazon_request = requests.post('https://www.amazon.com/s?k={}'.format(product), headers)\n",
    "    if 'robot' in amazon_request.text:\n",
    "        print('Product query failure: bot detected')\n",
    "    return amazon_request.text\n",
    "\n",
    "def parse_asins(items):\n",
    "    #items.count('asin')\n",
    "    unparsed_asins = [m.start() for m in re.finditer('asin', items)]\n",
    "    #print(len(unparsed_asins))\n",
    "    asins = []\n",
    "    for a in range(len(unparsed_asins)):\n",
    "        asin = items[unparsed_asins[a]+3:unparsed_asins[a]+18]\n",
    "        #print(asin, end = ':\\t')\n",
    "        _asin = asin.split('=')\n",
    "        if len(_asin) > 1:\n",
    "            _asin = _asin[1].split('\"')\n",
    "        else:\n",
    "            continue\n",
    "        if len(_asin) > 1:\n",
    "            _asin = _asin[1].split('&')[0]\n",
    "        else:\n",
    "            _asin = _asin[0].split('&')[0]\n",
    "        asin = _asin.split(',')[0]\n",
    "        #print(asin)\n",
    "        prog = re.compile(\"[A-Z][0-9]+[A-Z]*.*\")\n",
    "        if prog.match(asin) is not None:\n",
    "            asins.append(asin)\n",
    "            #print(asin, end='')\n",
    "        #print('')\n",
    "    return asins\n",
    "\n",
    "def get_product_reviews(asin):\n",
    "    amazon_url  = 'http://www.amazon.com/dp/'+asin\n",
    "    # Add some recent user agent to prevent amazon from blocking the request \n",
    "    # Find some chrome user agent strings  here https://udger.com/resources/ua-list/browser-detail?browser=Chrome\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.9) Gecko/2008051206 Firefox/3.0'}\n",
    "    page = requests.post(amazon_url,headers = headers,verify=False)\n",
    "    if 'robot' in page.text:\n",
    "        print('Product query failure: bot detected during review scraping')\n",
    "    if 'data-hook=\"review-collapsed\"' in page.text:\n",
    "        _reviews = [m.start() for m in re.finditer('data-hook=\"review-collapsed\"', page.text)]\n",
    "        a, b, reviews = 114, 10000, []\n",
    "        for r in range(len(_reviews)):\n",
    "            reviews.append(page.text[_reviews[r]+a:min(_reviews[r]+(a+b), len(page.text))].split(\"</div>\")[0])\n",
    "        return reviews\n",
    "    return None\n",
    "\n",
    "def get_salience(comment):\n",
    "    _url = 'https://language.googleapis.com/v1/documents:analyzeEntities?key=AIzaSyDHHCAG-BhRFaUxq2NRz2LG0tPiVNB4bos'\n",
    "    _data = '{{\\'document\\': {{\\'type\\': \\'PLAIN_TEXT\\', \\'content\\': \\'{}\\'}}, \\'encodingType\\': \\'UTF8\\'}}'.format(comment.encode(\"utf-8\").decode(\"utf-8\"))\n",
    "    r = requests.post(_url, data = _data)\n",
    "    return r.json()\n",
    "\n",
    "def get_sentiment(comment):\n",
    "    perspective_url = 'https://commentanalyzer.googleapis.com/v1alpha1/comments:analyze?key=AIzaSyDHHCAG-BhRFaUxq2NRz2LG0tPiVNB4bos'\n",
    "    perspective_data = '{{comment: {{text: \"{}\"}}, languages: [\"en\"], requestedAttributes: {{TOXICITY:{{}}}}}}'.format(comment.encode(\"utf-8\").decode(\"ascii\",\"ignore\"))\n",
    "    r = requests.post(perspective_url, data = perspective_data)\n",
    "    return r.json()\n",
    "\n",
    "def extract_single_review_sentiment(product, item, review):\n",
    "    #SANDWHICHONEAREYOU\n",
    "    #I hope you hate me as much as i do\n",
    "    return get_sentiment(get_product_reviews(parse_asins(get_products(product))[item])[review])\n",
    "\n",
    "def get_product_sentiment(product, product_count):\n",
    "    items = get_products(product)\n",
    "    asins = parse_asins(items)\n",
    "    item_reviews = []\n",
    "    for asin in asins[:product_count]:\n",
    "        #This is likely to get filtered for excessive querying\n",
    "        item_reviews.append(get_product_reviews(asin))\n",
    "    return item_reviews\n",
    "    '''salience = []\n",
    "    for item in item_reviews:\n",
    "        for review in item:\n",
    "            salience.append((get_sentiment(review), get_salience(review)))\n",
    "    return salience'''\n",
    "    \n",
    "#items = get_products(\"sprite\")\n",
    "#asins = parse_asins(items)\n",
    "#reviews = get_product_reviews(asins[1])\n",
    "#sentiment = get_sentiment(reviews[1])\n",
    "data = get_product_sentiment(\"soylent\", 2)\n",
    "data_string = ''\n",
    "for row in data:\n",
    "    for datum in row:\n",
    "        data_string += datum\n",
    "print(data_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev = \"This is a shitty review. I really like this bottle though.\"\n",
    "review_datum = (sentiment, salience) = (get_sentiment(rev), get_salience(rev))#['score']['value']\n",
    "#salience = get_salience(rev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in range(len(data[1]['entities'])):\n",
    "'''print('Word: {}\\nSalience: {}\\nSentiment: {}\\n'.format(data[1]['entities'][i]['name'], \n",
    "                                                         data[1]['entities'][i]['salience'], \n",
    "                                                         data[0]['attributeScores']['TOXICITY']['summaryScore']['value']))'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = {}\n",
    "print(len(data))\n",
    "for datum in data:\n",
    "    print(datum)\n",
    "    if 'entities' in datum[1]:\n",
    "        for i in range(len(review_datum[1]['entities'])):\n",
    "            print('Word: {}\\nSalience: {}\\nSentiment: {}\\n'.format(review_datum[1]['entities'][i]['name'], \n",
    "                                                         review_datum[1]['entities'][i]['salience'], \n",
    "                                                         review_datum[0]['attributeScores']['TOXICITY']['summaryScore']['value']))\n",
    "            #print('{}: {}'.format(word['name'], word['salience']))\n",
    "            #keywords.update({word['name']:word['salience']})\n",
    "#import operator\n",
    "#sorted_kw = sorted(keywords.items(), key=operator.itemgetter(1))[::-1]\n",
    "#print(sorted_kw[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The\n",
    "'''\n",
    "curl -H \"Content-Type: application/json\" --data     '{comment: {text: \"This is really stupid.\"},\n",
    "      languages: [\"en\"],\n",
    "      requestedAttributes: {TOXICITY:{}} }'     https://commentanalyzer.googleapis.com/v1alpha1/comments:analyze?key=AIzaSyDHHCAG-BhRFaUxq2NRz2LG0tPiVNB4bos\n",
    "\n",
    "'''\n",
    "\n",
    "comment = review#'Raquel'\n",
    "perspective_url = 'https://commentanalyzer.googleapis.com/v1alpha1/comments:analyze?key=AIzaSyDHHCAG-BhRFaUxq2NRz2LG0tPiVNB4bos'\n",
    "perspective_data = '{{comment: {{text: \"{}\"}}, languages: [\"en\"], requestedAttributes: {{TOXICITY:{{}}}}}}'.format(comment)\n",
    "r = requests.post(perspective_url, data = perspective_data)\n",
    "r.text      # response as a string\n",
    "r.content   # response as a byte string\n",
    "            #     gzip and deflate transfer-encodings automatically decoded \n",
    "r.json()    # return python object from json! this is what you probably want!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
